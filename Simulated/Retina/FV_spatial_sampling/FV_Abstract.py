import torch.nn as nn
from root_config import *
from abc import ABC, abstractmethod

class AbstractSpatialSampling(nn.Module, ABC):
    def __init__(self, params, device):
        super(AbstractSpatialSampling, self).__init__()

        # Requirement: this function should return an integer that defines the required image resolution.
        self.required_image_resolution = -1

    @abstractmethod
    def forward(self, image):

        # Requirement: output should be a tensor of shape (BS, C, params['RetinaModel']['ons_dim'], params['RetinaModel']['ons_dim'])

        # Input: unwarped image (BS, C, uH, uW)
        #        BS is the batch size, C is the number of channels, uH is the unwarped height, and uW is the unwarped width
        # Output: foveated image (BS, C, params['RetinaModel']['ons_dim'], params['RetinaModel']['ons_dim'])
        #        params['RetinaModel']['ons_dim'] is the number of cones in height and width simulated in the retina.
        # If you are simulating the foveation, uH and uW are usually larger than params['RetinaModel']['ons_dim'].
        # For our base model, we use uH = 512, uW = 512, params['RetinaModel']['ons_dim'] = 256.
        # So the 256x256 foveated image is generated by sampling from the 512x512 original image.
        
        pass
